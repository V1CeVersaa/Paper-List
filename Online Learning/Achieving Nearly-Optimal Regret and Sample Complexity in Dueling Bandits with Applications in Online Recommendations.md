# Achieving Nearly-Optimal Regret and Sample Complexity in Dueling Bandits with Applications in Online Recommendations

**3.2 RM is nearly-consistent with BAI in dueling bandits**

关键点在于：

- 为了最小化累计遗憾，我们需要尽可能快的将 Condorcet Winner 找出来；
- BAI 也是希望尽快的将 Condorcet Winner 找出来，而比较两个摇杆的样本复杂度恰好是分别两个摇杆的概率的平方反比，这也要求我们将 Condorcet Winner 尽快找出来。



